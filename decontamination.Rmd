---
title: "decontamination of pcr replicates and ASVs for rockfish dloop mock communities and bottom trawl samples"
author: "Kimberly Ledger"
date: "2024-11-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

libraries
```{r}
library(tidyverse)
rename <- dplyr::rename
library(reshape2)
```

load sample metadata
```{r}
metadata <- read.csv("/home/kimberly.ledger/rockfish_mb/data/mock_bt22_metadata.csv") %>%
  rename(Sample_ID = sample_ID) %>%
  mutate(sample_type = ifelse(Sample_ID == "PC", "pcr_blank", sample_type),
         sample_type = ifelse(Sample_ID == "NC", "positive", sample_type))    ### if flip-flopped these IDs 
```

check sequence table outputs
```{r}
asv_table <- readRDS("/home/kimberly.ledger/rockfish_mb/data/blastn_output/filtered.seqTab.RDS") %>%
  select(!Row.names)

#transpose 
asv_table <- data.frame(t(asv_table))

#set column names to be ASV# 
colnames(asv_table) <- asv_table["ASV",]

#remove row that has ASV#
asv_table <- asv_table[!rownames(asv_table) %in% c('ASV'), ]

#make sure reads are numbers
# Convert all character columns to numeric
for (col in names(asv_table)) {
  asv_table[[col]] <- as.numeric(asv_table[[col]])
}

#make make sample ID a column 
asv_table$Sample_ID <- rownames(asv_table)

asv_table <- asv_table %>%
  select(Sample_ID, everything()) %>%
  mutate(across(everything(), ~ gsub("-", "_", .))) %>%
  mutate(Sample_ID = ifelse(Sample_ID == "NC22_1_A", "NC22-1_A", Sample_ID)) %>%
  mutate(Sample_ID = ifelse(Sample_ID == "NC22_2_A", "NC22-2_A", Sample_ID))
```

add column to the ASV table that labels the sample type
```{r}
asv_table_with_sample_type <- metadata %>%
  dplyr::select(Sample_ID, sample_type) %>%
  left_join(asv_table, by = "Sample_ID")

# make a variable for the first and last ASV column in the table
asv_first <- which(startsWith(names(asv_table_with_sample_type), "ASV"))[1]
asv_last <- ncol(asv_table_with_sample_type)
```


# account for likely contaminants 

step 1: tag-jumping: subtract the proportion of reads that jumped into the positive control samples from each environmental sample 
step 2: remove ASVs that don't get a fish taxonomic assignment 
step 3: remove any ASVs that don't show up in field samples 
step 4: remove low read depth samples based on ASV accumulation curve  
step 5: check dissimilarity across PCR replicates and remove any replicates with high dissimilarity 


## Step 1. Account for tag-jumping by using the positive controls 

subtract the proportion of reads that jumped into the positive control samples from each environmental sample 

plot the positive controls
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  mutate(reads = as.numeric(reads)) %>%
  mutate(reads = ifelse(is.na(reads), 0, reads)) %>%
  filter(sample_type %in% c("positive")) %>%
  filter(reads > 0) %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - positive controls") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    #legend.position = "none",
    legend.title = element_blank()
  )
```

identify the maximum proportion of reads for each ASV found in the positive controls
note: can only base this off PC22_A since the PC in the other plate was pop and that's a target species 
```{r}
prop_asvs_in_positives <- asv_table_with_sample_type %>%
  filter(Sample_ID == "PC22_A") %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  mutate(reads = as.numeric(reads)) %>%
  group_by(Sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop))
```

subtract the max proportion of tag-jumped reads for each ASV from samples
```{r}
indexhop_table <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  mutate(reads = as.numeric(reads)) %>%
  mutate(reads = ifelse(is.na(reads), 0, reads)) %>%
  group_by(Sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads, na.rm = T)) %>%
  left_join(prop_asvs_in_positives, by = c("ASV")) %>%
  mutate(IndexHoppingReads = TotalReadsPerSample*max_prop) %>%
  mutate(reads_IndexHop_removed = reads - IndexHoppingReads) %>%
  mutate(reads_IndexHop_removed = if_else(reads_IndexHop_removed < 0, 0, reads_IndexHop_removed))
```

clean up the table by removing columns no longer needed 
```{r}
asv_table_filter1 <- indexhop_table %>%
  dplyr::select(Sample_ID, sample_type, ASV, reads_IndexHop_removed) %>%
  dplyr::rename(reads = reads_IndexHop_removed)
```

this is a summary of the number of reads removed by ASV and sample_ID
```{r}
decontaminated_1 <- indexhop_table %>%
  dplyr::select(Sample_ID, ASV, IndexHoppingReads) %>%
  pivot_wider(names_from = "ASV", values_from = "IndexHoppingReads")
```

## Step 2. Remove ASVs that don't get a taxonomic assignment  

```{r}
taxonomy <- read.csv("/home/kimberly.ledger/rockfish_mb/data/blastn_output/taxonomy_mockandbt.csv") %>% 
  select(!X) %>%
  rename(ASV = qseqid)

asv_table_filter2 <- asv_table_filter1 %>%
  filter(ASV %in% taxonomy$ASV)
```


## Step 3. NOT DOING THIS!!!!! 
Remove ASVs that do not occur in environmental samples or mock communities 

```{r}
reads_per_type_ASV <- asv_table_filter2 %>%
  group_by(ASV, sample_type) %>%
  summarize(TotalReadsPerASV = sum(reads, na.rm = TRUE)) %>%
  arrange(ASV)
```

what ASVs have no reads in samples or mock communities, but reads in the controls? 
```{r}
not_in_samples <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
    filter(sample < 1 & mock < 1 & tissue < 1)
not_in_samples
```

these are all pop and northern asvs, which would make sense for field contaimination 

remove these from the asv table
```{r}
#asv_table_filter3 <- asv_table_filter2 %>%
#  filter(!ASV %in% not_in_samples$ASV)
```


## Step 4. Remove low read depth samples based on ASV accumulation curve

```{r}
library(vegan)

asv_table_wide <- asv_table_filter2 %>%
  select(!sample_type) %>%
  mutate(reads = as.integer(reads)) %>%
  pivot_wider(names_from = ASV, values_from = reads)

sample_IDs <- asv_table_wide$Sample_ID

asv_table_wide <- asv_table_wide %>%
  ungroup() %>%
  select(-Sample_ID)

## plots the figure
rarecurve(asv_table_wide, step = 20, col = "blue", label = FALSE, 
          main = "Sequencing Effort Curves",
          xlab = "Sequencing Depth", ylab = "Number of ASVs Identified",
          xlim = c(0,5000))
```

summarize in a table how many pcr replicates meet certain read count thresholds 
```{r}
read_summary <- asv_table_filter2 %>%
  group_by(Sample_ID, sample_type) %>%
  summarize(tot_reads = sum(reads)) %>%
  arrange(desc(tot_reads)) %>%
  group_by(sample_type) %>%
  summarize(atleast1 = sum(tot_reads >= 1),
            atleast250 = sum(tot_reads >= 250),
            atleast500 = sum(tot_reads >= 500),
            atleast750 = sum(tot_reads >= 750),
            atleast1k = sum(tot_reads >= 1000),
            atleast2k = sum(tot_reads >= 2000))
```

based on taxa accumulation curve and summary table, we will remove any pcr replicate with fewer than 1000 reads from downstream analyses

```{r}
reps_below <- asv_table_filter2 %>%
  group_by(Sample_ID) %>%
  summarise(tot_reads = sum(reads)) %>%
  filter(tot_reads < 1000)
```

```{r}
asv_table_filter4 <- asv_table_filter2 %>%
  filter(!Sample_ID %in% reps_below$Sample_ID)
```


```{r}
write.csv(asv_table_filter4, "/home/kimberly.ledger/rockfish_mb/data/decontamination/decontaminated_asv_table.csv")
```


